#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "httpx>=0.25.0",
#     "beautifulsoup4>=4.12.0", 
#     "aiofiles>=23.0.0",
#     "jinja2>=3.1.0",
#     "tqdm>=4.65.0",
#     "platformdirs>=4.0.0",
# ]
# ///
"""
Firefox History Scraper with Content Caching

Extracts URLs from Firefox history, caches scraped content, and generates HTML reports.
Supports incremental updates, offline keyword searching, and intelligent exclusion filtering.

Features:
- Default exclusions for social media, streaming, ads, and non-text content
- Custom exclusion patterns for domains and file extensions
- Cross-platform Firefox profile detection

Usage:
    ./scrape-history "python tutorial"                              # Basic search (uses defaults)
    ./scrape-history --days 30 --exclude "reddit" "machine learning" # Add custom exclusions
    ./scrape-history --include "github.com" "code review"           # Include GitHub despite defaults
    ./scrape-history --include ".pdf" --exclude "amazon" "research" # Include PDFs, exclude Amazon
    ./scrape-history --no-default-exclusions "github issue"         # Disable default exclusions
    ./scrape-history --show-exclusions                             # Show active filtering patterns
    ./scrape-history --search-cache "python" "machine learning"    # Search existing cache only
    ./scrape-history --refresh-cache --days 1                      # Force refresh recent URLs
    ./scrape-history --list-profiles                               # Show available Firefox profiles
    ./scrape-history --profile "Work*" --days 3 "meeting notes"    # Use specific profile pattern
"""

import argparse
import sqlite3
import asyncio
import re
import hashlib
import json
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urlparse
from typing import List, Dict, Set, Optional
import platform
import os
import logging
import warnings

import httpx
from bs4 import BeautifulSoup
import aiofiles
from jinja2 import Template
from tqdm import tqdm
from platformdirs import user_cache_dir, user_data_dir

# Suppress BeautifulSoup encoding warnings
warnings.filterwarnings("ignore", message="Some characters could not be decoded")
warnings.filterwarnings("ignore", message=".*REPLACEMENT CHARACTER.*")


# Default exclusion patterns
DEFAULT_DOMAIN_EXCLUSIONS = [
    # Search engines
    "google.com", "kagi.com", "bing.com", "duckduckgo.com",
    
    # Social media platforms
    "youtube.com", "twitter.com", "facebook.com", "instagram.com", 
    "linkedin.com", "tiktok.com", "snapchat.com", "pinterest.com",
    
    # Code hosting (user requested GitHub)
    "github.com",
    
    # Streaming services
    "netflix.com", "disneyplus.com", "hulu.com", "amazon.com/prime",
    "spotify.com", "apple.com/tv", "hbomax.com", "paramount.com",
    
    # E-commerce product pages (common patterns)
    "amazon.com/dp/", "amazon.com/gp/", "amazon.co.uk/dp/", 
    "amazon.de/dp/", "amazon.fr/dp/", "amazon.com.au/dp/",
    "ebay.com/itm/", "etsy.com/listing/",
    
    # Online stores/shopping
    "shopify.com", "woocommerce.com", "bigcommerce.com",
    
    # Ad/tracking domains
    "doubleclick.net", "googletagmanager.com", "googlesyndication.com",
    "facebook.com/tr", "analytics.google.com",
]

DEFAULT_FILE_EXTENSION_EXCLUSIONS = [
    # Images
    ".jpg", ".jpeg", ".png", ".gif", ".webp", ".svg", ".bmp", ".ico",
    
    # Documents that are hard to search meaningfully
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
    
    # Stylesheets and assets
    ".css", ".scss", ".sass", ".less",
    
    # Archives and binaries
    ".zip", ".tar", ".gz", ".rar", ".7z", ".exe", ".dmg", ".deb", ".rpm",
    
    # Media files
    ".mp3", ".mp4", ".avi", ".mov", ".wmv", ".flv", ".webm", ".m4a",
    
    # Fonts
    ".woff", ".woff2", ".ttf", ".otf", ".eot",
]


def get_effective_patterns(custom_exclusions: List[str], custom_inclusions: List[str], use_defaults: bool = True) -> Dict[str, Dict[str, List[str]]]:
    """Combine default and custom exclusion/inclusion patterns."""
    exclusions = {'domains': [], 'extensions': [], 'general': []}
    inclusions = {'domains': [], 'extensions': [], 'general': []}
    
    # Add default exclusions if enabled
    if use_defaults:
        exclusions['domains'].extend(DEFAULT_DOMAIN_EXCLUSIONS)
        exclusions['extensions'].extend(DEFAULT_FILE_EXTENSION_EXCLUSIONS)
    
    # Add custom exclusions
    for pattern in custom_exclusions:
        if pattern.startswith('.'):
            exclusions['extensions'].append(pattern)
        else:
            exclusions['general'].append(pattern)
    
    # Add custom inclusions (overrides for exclusions)
    for pattern in custom_inclusions:
        if pattern.startswith('.'):
            inclusions['extensions'].append(pattern)
        else:
            inclusions['general'].append(pattern)
    
    return {
        'exclusions': exclusions,
        'inclusions': inclusions
    }


def should_exclude_url(url: str, patterns: Dict[str, Dict[str, List[str]]]) -> bool:
    """Check if URL should be excluded based on exclusion/inclusion patterns."""
    url_lower = url.lower()
    
    exclusions = patterns['exclusions']
    inclusions = patterns['inclusions']
    
    # First check inclusions - if URL matches any include pattern, always allow it
    # Check include domain patterns
    for domain in inclusions['domains']:
        if domain.lower() in url_lower:
            return False
    
    # Check include file extension patterns  
    for ext in inclusions['extensions']:
        if url_lower.endswith(ext.lower()):
            return False
    
    # Check include general patterns
    for pattern in inclusions['general']:
        if pattern.lower() in url_lower:
            return False
    
    # Now check exclusions - only exclude if not already included
    # Check exclude domain patterns
    for domain in exclusions['domains']:
        if domain.lower() in url_lower:
            return True
    
    # Check exclude file extension patterns
    for ext in exclusions['extensions']:
        if url_lower.endswith(ext.lower()):
            return True
    
    # Check exclude general patterns (legacy behavior)
    for pattern in exclusions['general']:
        if pattern.lower() in url_lower:
            return True
    
    return False


def check_include_warnings(include_patterns: List[str], exclusions: Dict[str, List[str]]):
    """Check include patterns for potential issues and show helpful warnings."""
    if not include_patterns:
        return
    
    # Collect all exclusion patterns into a single list for checking
    all_exclusions = []
    all_exclusions.extend(exclusions['domains'])
    all_exclusions.extend(exclusions['extensions'])
    all_exclusions.extend(exclusions['general'])
    
    for pattern in include_patterns:
        # Only warn for short patterns that are likely to be overly broad
        if len(pattern) < 6:
            matches = [excl for excl in all_exclusions if pattern.lower() in excl.lower()]
            if matches:
                print(f"Warning: Include pattern '{pattern}' may match unintended URLs.")
                print(f"    Found related exclusions: {', '.join(matches[:3])}")  # Show up to 3 matches
                if len(matches) == 1:
                    print(f"    Did you mean: --include \"{matches[0]}\"?")
                elif len(matches) > 1:
                    print(f"    Did you mean one of: {' or '.join('--include \"' + m + '\"' for m in matches[:2])}?")
                print()


# Cross-platform utilities (inspired by browserexport)
def determine_operating_system() -> str:
    """Determine OS with WSL detection."""
    # WSL detection
    procfile = Path("/proc/version")
    if procfile.exists():
        proc_version = procfile.read_text()
        if "microsoft" in proc_version.lower():
            return "win32"
    return platform.system().lower()


def windows_appdata_paths(path: str) -> tuple[str, ...]:
    """Expand Windows APPDATA paths."""
    base_vars = ("%LOCALAPPDATA%", "%APPDATA%")
    return tuple(
        os.path.normpath(os.path.expandvars(os.path.join(var, path)))
        for var in base_vars
    )


def expand_path(path_str: str) -> Path:
    """Expand user path and make absolute."""
    return Path(path_str).expanduser().absolute()


def handle_path(pathmap: dict[str, str | tuple[str, ...]], browser_name: str = "Firefox") -> tuple[Path, ...]:
    """Cross-platform path resolution."""
    os_key = determine_operating_system()
    
    # Map different OS names
    if os_key == "darwin":
        os_key = "darwin"
    elif os_key in ("linux", "linux2"):
        os_key = "linux"
    elif os_key in ("win32", "windows"):
        os_key = "win32"
    
    paths = pathmap.get(os_key)
    if not paths:
        # Fallback to linux behavior
        print(f"Warning: {browser_name} paths not defined for {os_key}, using Linux defaults")
        paths = pathmap.get("linux", pathmap[list(pathmap.keys())[0]])
    
    # Normalize to tuple
    if isinstance(paths, str):
        paths = (paths,)
    
    return tuple(expand_path(p) for p in paths)


def handle_glob(base_paths: tuple[Path, ...], pattern: str) -> Path:
    """Find database using glob pattern with recursive fallback."""
    from itertools import chain
    
    def try_glob(recursive: bool = False) -> list[Path]:
        results = []
        for base in base_paths:
            if not base.exists():
                continue
            if recursive:
                results.extend(base.rglob(pattern))
            else:
                results.extend(base.glob(pattern))
        return results
    
    # Try non-recursive first
    matches = try_glob(recursive=False)
    if not matches:
        # Try recursive as fallback
        matches = try_glob(recursive=True)
    
    if len(matches) > 1:
        paths_str = "\n".join(str(p) for p in matches)
        raise ValueError(f"Multiple Firefox profiles found:\n{paths_str}\n\nUse --profile to specify one")
    elif len(matches) == 1:
        return matches[0]
    else:
        raise FileNotFoundError(f"No Firefox database found matching pattern '{pattern}' in {base_paths}")


# Browser classes (simplified browserexport approach)
class Browser:
    """Base browser class."""
    
    @classmethod
    def data_directories(cls) -> tuple[Path, ...]:
        """Get browser data directories for current platform."""
        raise NotImplementedError
    
    @classmethod
    def locate_database(cls, profile: str = "*") -> Path:
        """Locate database file using profile pattern."""
        raise NotImplementedError


class Firefox(Browser):
    """Firefox browser implementation."""
    
    @classmethod
    def data_directories(cls) -> tuple[Path, ...]:
        """Get Firefox profile directories."""
        return handle_path({
            "linux": (
                "~/.mozilla/firefox/",
                "~/.var/app/org.mozilla.firefox/.mozilla/firefox/",  # Flatpak
                "~/snap/firefox/common/.mozilla/firefox/",  # Snap
            ),
            "darwin": "~/Library/Application Support/Firefox/Profiles/",
            "win32": windows_appdata_paths("Mozilla/Firefox/Profiles/"),
        })
    
    @classmethod
    def locate_database(cls, profile: str = "*") -> Path:
        """Locate Firefox places.sqlite using profile pattern."""
        dirs = cls.data_directories()
        return handle_glob(dirs, f"{profile}/places.sqlite")


class ContentCache:
    """Persistent cache for scraped web content.
    
    Uses platformdirs for proper cross-platform cache directory management:
    - macOS: ~/Library/Caches/scrape-history  
    - Linux: ~/.cache/scrape-history
    - Windows: %LOCALAPPDATA%\\scrape-history
    """
    
    def __init__(self, cache_dir: Path = None):
        if cache_dir:
            self.cache_dir = cache_dir
        else:
            # Use platformdirs for proper cross-platform cache directory
            self.cache_dir = Path(user_cache_dir("scrape-history", "kwbr"))
        
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.db_path = self.cache_dir / "content_cache.db"
        self._init_db()
    
    def _init_db(self):
        """Initialize cache database."""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS cached_content (
                url_hash TEXT PRIMARY KEY,
                url TEXT NOT NULL,
                title TEXT,
                content TEXT,
                scraped_at TIMESTAMP,
                content_hash TEXT,
                status TEXT,
                error_message TEXT
            )
        """)
        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_url ON cached_content(url);
        """)
        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_scraped_at ON cached_content(scraped_at);
        """)
        conn.commit()
        conn.close()
    
    def _url_hash(self, url: str) -> str:
        """Generate hash for URL."""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _content_hash(self, content: str) -> str:
        """Generate hash for content."""
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_cached_content(self, url: str, max_age_hours: int = 24) -> Optional[Dict]:
        """Get cached content if it exists and is fresh enough."""
        url_hash = self._url_hash(url)
        cutoff_time = (datetime.now() - timedelta(hours=max_age_hours)).isoformat()
        
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT * FROM cached_content 
            WHERE url_hash = ? AND scraped_at > ?
        """, (url_hash, cutoff_time))
        
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return {
                'url': row['url'],
                'title': row['title'],
                'content': row['content'],
                'scraped_at': datetime.fromisoformat(row['scraped_at']),
                'status': row['status'],
                'error': row['error_message'],
                'from_cache': True
            }
        
        return None
    
    def cache_content(self, url: str, title: str, content: str, status: str, error: str = None):
        """Store content in cache."""
        url_hash = self._url_hash(url)
        content_hash = self._content_hash(content) if content else None
        
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            INSERT OR REPLACE INTO cached_content 
            (url_hash, url, title, content, scraped_at, content_hash, status, error_message)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (url_hash, url, title, content, datetime.now().isoformat(), 
              content_hash, status, error))
        conn.commit()
        conn.close()
    
    def search_cached_content(self, keywords: List[str], patterns: Dict[str, Dict[str, List[str]]] = None) -> List[Dict]:
        """Search through all cached content for keywords."""
        patterns = patterns or {'exclusions': {'domains': [], 'extensions': [], 'general': []}, 'inclusions': {'domains': [], 'extensions': [], 'general': []}}
        
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT url, title, content, scraped_at, status
            FROM cached_content 
            WHERE status = 'success' AND content IS NOT NULL
            ORDER BY scraped_at DESC
        """)
        
        results = []
        keyword_searcher = KeywordSearcher()
        
        for row in cursor.fetchall():
            url = row['url']
            
            # Apply exclusion filters using new logic
            if should_exclude_url(url, patterns):
                continue
            
            matches = keyword_searcher.search_content(row['content'], keywords)
            if matches:
                results.append({
                    'url': url,
                    'title': row['title'],
                    'visited': datetime.fromisoformat(row['scraped_at']),
                    'matches': matches,
                    'status': 'success',
                    'from_cache': True
                })
        
        conn.close()
        return results
    
    def get_cache_stats(self) -> Dict:
        """Get cache statistics."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM cached_content")
        total_entries = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM cached_content WHERE status = 'success'")
        successful_entries = cursor.fetchone()[0]
        
        cursor.execute("SELECT MIN(scraped_at), MAX(scraped_at) FROM cached_content")
        min_date, max_date = cursor.fetchone()
        
        conn.close()
        
        # Calculate cache size in MB
        cache_size_mb = 0.0
        if self.db_path.exists():
            cache_size_bytes = self.db_path.stat().st_size
            cache_size_mb = cache_size_bytes / (1024 * 1024)  # Convert to MB
        
        return {
            'total_entries': total_entries,
            'successful_entries': successful_entries,
            'oldest_entry': min_date,
            'newest_entry': max_date,
            'cache_size_mb': cache_size_mb
        }
    
    def clean_old_entries(self, days: int = 30):
        """Remove entries older than specified days."""
        cutoff_time = (datetime.now() - timedelta(days=days)).isoformat()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM cached_content WHERE scraped_at < ?", 
                      (cutoff_time,))
        deleted = cursor.rowcount
        conn.commit()
        conn.close()
        
        return deleted


class FirefoxHistoryExtractor:
    """Extract URLs from Firefox history database using browserexport-style detection."""
    
    def __init__(self):
        self.browser = Firefox()
    
    def list_profiles(self) -> List[Dict]:
        """List available Firefox profiles with glob pattern examples."""
        try:
            dirs = self.browser.data_directories()
            profiles = []
            
            for base_dir in dirs:
                if not base_dir.exists():
                    continue
                
                # Find all profile directories with places.sqlite
                for profile_dir in base_dir.iterdir():
                    if profile_dir.is_dir():
                        places_db = profile_dir / "places.sqlite"
                        if places_db.exists():
                            profiles.append({
                                'directory': profile_dir.name,
                                'name': profile_dir.name,  # Use directory name as profile name
                                'path': places_db,
                                'is_default': 'default' in profile_dir.name.lower()
                            })
            
            return sorted(profiles, key=lambda p: (not p['is_default'], p['directory']))
            
        except Exception as e:
            raise FileNotFoundError(f"Failed to list Firefox profiles: {e}")
    
    def get_profile_path(self, profile_pattern: str = None) -> Path:
        """Get database path using profile pattern (supports glob patterns)."""
        pattern = profile_pattern or "*"
        try:
            return self.browser.locate_database(pattern)
        except (ValueError, FileNotFoundError) as e:
            # Provide helpful error with available profiles
            try:
                profiles = self.list_profiles()
                available = [p['directory'] for p in profiles]
                raise ValueError(f"{e}\nAvailable profiles: {available}")
            except:
                raise e
    
    def extract_urls(self, days: int, patterns: Dict[str, Dict[str, List[str]]], profile_pattern: str = None) -> List[Dict]:
        """Extract URLs from Firefox history using profile pattern."""
        db_path = self.get_profile_path(profile_pattern)
        
        # Calculate date threshold
        since_date = datetime.now() - timedelta(days=days)
        since_timestamp = int(since_date.timestamp() * 1_000_000)  # Firefox uses microseconds
        
        conn = sqlite3.connect(f"file:{db_path}?mode=ro&immutable=1", uri=True)
        cursor = conn.cursor()
        
        query = """
        SELECT DISTINCT url, title, last_visit_date, visit_count
        FROM moz_places 
        WHERE last_visit_date > ? 
        AND url NOT LIKE 'about:%'
        AND url NOT LIKE 'moz-extension:%'
        AND visit_count > 0
        ORDER BY last_visit_date DESC
        """
        
        cursor.execute(query, (since_timestamp,))
        results = []
        
        for url, title, timestamp, visit_count in cursor.fetchall():
            # Apply exclusion filters using new logic
            if should_exclude_url(url, patterns):
                continue
            
            results.append({
                'url': url,
                'title': title or url,
                'visited': datetime.fromtimestamp(timestamp / 1_000_000),
                'visit_count': visit_count
            })
        
        conn.close()
        return results


class AsyncWebScraper:
    """Async web scraper with caching."""
    
    def __init__(self, cache: ContentCache, max_concurrent: int = 10):
        self.cache = cache
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.client = httpx.AsyncClient(
            timeout=30.0,
            headers={'User-Agent': 'Kai\'s vibecode project'},
            follow_redirects=True
        )
    
    async def scrape_url(self, url: str, use_cache: bool = True, max_cache_age_hours: int = 24) -> Dict:
        """Scrape a URL, using cache when possible."""
        
        # Try cache first
        if use_cache:
            cached = self.cache.get_cached_content(url, max_cache_age_hours)
            if cached:
                return cached
        
        # Scrape fresh content
        async with self.semaphore:
            try:
                response = await self.client.get(url)
                response.raise_for_status()
                
                # Handle encoding more gracefully
                content = response.content
                try:
                    # Try to decode with response encoding first
                    text_content = content.decode(response.encoding or 'utf-8', errors='replace')
                except (UnicodeDecodeError, LookupError):
                    # Fallback to utf-8 with replacement
                    text_content = content.decode('utf-8', errors='replace')
                
                # Parse with warnings suppressed
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    soup = BeautifulSoup(text_content, 'html.parser')
                
                # Remove script and style elements
                for script in soup(["script", "style", "nav", "header", "footer", ".advertisement"]):
                    script.decompose()
                
                # Extract text content
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                title = soup.title.string if soup.title else ''
                
                # Cache the successful result
                self.cache.cache_content(url, title, text, 'success')
                
                return {
                    'url': url,
                    'title': title,
                    'content': text,
                    'status': 'success',
                    'from_cache': False
                }
                
            except Exception as e:
                error_msg = str(e)
                
                # Cache the error (with empty content)
                self.cache.cache_content(url, '', '', 'error', error_msg)
                
                return {
                    'url': url,
                    'title': '',
                    'content': '',
                    'status': 'error',
                    'error': error_msg,
                    'from_cache': False
                }
    
    async def close(self):
        await self.client.aclose()


class KeywordSearcher:
    """Search for keywords in scraped content."""
    
    def search_content(self, content: str, keywords: List[str], max_context_length: int = 300, max_matches_per_keyword: int = 10) -> List[Dict]:
        """Find keyword matches with context."""
        matches = []
        
        # Limit content length for performance (keep first 50KB)
        if len(content) > 50000:
            content = content[:50000] + "... [content truncated for performance]"
        
        for keyword in keywords:
            # Support both exact and fuzzy matching
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            keyword_matches = 0
            
            for match in pattern.finditer(content):
                if keyword_matches >= max_matches_per_keyword:
                    break
                
                # Smaller context window for better performance
                context_window = min(200, max_context_length // 2)
                start = max(0, match.start() - context_window)
                end = min(len(content), match.end() + context_window)
                context = content[start:end].strip()
                
                # Clean up context and limit length
                context = re.sub(r'\s+', ' ', context)
                if len(context) > max_context_length:
                    context = context[:max_context_length] + "..."
                
                matches.append({
                    'keyword': keyword,
                    'context': context,
                    'position': match.start()
                })
                keyword_matches += 1
        
        # Limit total matches per URL for performance
        return matches[:50]


class HTMLReportGenerator:
    """Generate HTML report with search results."""
    
    HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>Firefox History Search Results</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }
        .summary { background: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
        .cache-stats { background: #e8f4fd; padding: 10px; border-radius: 5px; font-size: 14px; margin-bottom: 20px; }
        .result { border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; }
        .result-header { background: #f8f9fa; padding: 15px; border-bottom: 1px solid #ddd; }
        .result-title { font-size: 18px; font-weight: bold; margin-bottom: 5px; }
        .result-url { color: #666; font-size: 14px; word-break: break-all; }
        .matches { padding: 15px; }
        .match { background: #fff3cd; padding: 10px; margin: 10px 0; border-radius: 3px; }
        .keyword { background: #ffc107; padding: 2px 4px; border-radius: 2px; font-weight: bold; }
        .error { color: #dc3545; font-style: italic; }
        .meta { color: #6c757d; font-size: 12px; }
        .cache-indicator { 
            display: inline-block; 
            padding: 2px 6px; 
            border-radius: 3px; 
            font-size: 11px; 
            font-weight: bold;
        }
        .cached { background: #d4edda; color: #155724; }
        .fresh { background: #cce5ff; color: #004085; }
        .search-box { margin-bottom: 20px; }
        .search-box input { padding: 8px; width: 300px; margin-right: 10px; }
        .search-box button { padding: 8px 15px; }
        
        /* Search highlighting */
        .highlight { background: #ffeb3b; padding: 1px 2px; }
    </style>
    <script>
        function highlightKeywords() {
            try {
                const keywords = {{ keywords | tojson }};
                const results = document.querySelectorAll('.match');
                
                results.forEach((result, index) => {
                    let html = result.innerHTML;
                    keywords.forEach(keyword => {
                        // Escape special regex characters
                        const escapedKeyword = keyword.replace(/[.*+?^${}()|\\[\\]\\\\]/g, '\\$&');
                        const regex = new RegExp(`(${escapedKeyword})`, 'gi');
                        html = html.replace(regex, '<span class="highlight">$1</span>');
                    });
                    result.innerHTML = html;
                });
            } catch (error) {
                console.error('Error during keyword highlighting:', error);
            }
        }
        
        function filterResults() {
            const searchTerm = document.getElementById('search').value.toLowerCase();
            const results = document.querySelectorAll('.result');
            let visibleCount = 0;
            
            results.forEach(result => {
                const text = result.textContent.toLowerCase();
                const isVisible = text.includes(searchTerm);
                result.style.display = isVisible ? 'block' : 'none';
                if (isVisible) visibleCount++;
            });
            
            // Update visible count
            document.getElementById('visible-count').textContent = visibleCount;
        }
        
        function toggleHighlighting() {
            const checkbox = document.getElementById('enable-highlighting');
            if (checkbox.checked) {
                highlightKeywords();
            } else {
                // Remove highlighting
                document.querySelectorAll('.highlight').forEach(el => {
                    el.outerHTML = el.innerHTML;
                });
            }
        }
        
        window.onload = function() {
            try {
                // Set up highlighting toggle
                document.getElementById('enable-highlighting').addEventListener('change', toggleHighlighting);
                
                // Initial highlighting
                highlightKeywords();
            } catch (error) {
                console.error('Error during initialization:', error);
            }
        }
    </script>
</head>
<body>
    <h1>Firefox History Search Results</h1>
    
    <div class="cache-stats">
        <strong>Cache Status:</strong> 
        {{ cache_stats.successful_entries }} pages cached ({{ "%.1f"|format(cache_stats.cache_size_mb) }} MB) | 
        Cache covers {{ cache_stats.oldest_entry }} to {{ cache_stats.newest_entry }}
    </div>
    
    <div class="summary">
        <strong>Search Summary:</strong><br>
        Keywords: {{ keywords | join(', ') }}<br>
        {% if days %}Time period: Last {{ days }} days<br>{% endif %}
        URLs processed: {{ total_urls }}<br>
        URLs with matches: {{ successful_matches }}<br>
        Failed scrapes: {{ failed_scrapes }}<br>
        From cache: {{ from_cache_count }} | Fresh scrapes: {{ fresh_scrape_count }}
    </div>
    
    <div class="search-box">
        <input type="text" id="search" placeholder="Filter results..." onkeyup="filterResults()">
        <button onclick="filterResults()">Filter</button>
        <label><input type="checkbox" id="enable-highlighting" checked> Enable keyword highlighting</label>
    </div>
    
    
    <div id="results-counter" style="margin-bottom: 10px; font-weight: bold;">
        Displaying <span id="visible-count">{{ results|length }}</span> of {{ results|length }} results
        {% if results_limited %}
        <div style="color: #dc3545; font-size: 14px; margin-top: 5px;">
            ⚠️ Results limited to 100 for performance. Use --max-urls to reduce total URLs processed.
        </div>
        {% endif %}
    </div>
    
    {% for result in results %}
    <div class="result">
        <div class="result-header">
            <div class="result-title">
                <a href="{{ result.url }}" target="_blank">{{ result.title or result.url }}</a>
                {% if result.from_cache %}
                <span class="cache-indicator cached">CACHED</span>
                {% else %}
                <span class="cache-indicator fresh">FRESH</span>
                {% endif %}
            </div>
            <div class="result-url">{{ result.url }}</div>
            <div class="meta">
                {% if result.visited %}Visited: {{ result.visited.strftime('%Y-%m-%d %H:%M') }} | {% endif %}
                Scraped: {{ result.get('scraped_at', 'Unknown') }}
            </div>
        </div>
        
        {% if result.status == 'error' %}
        <div class="matches">
            <div class="error">Failed to scrape: {{ result.get('error', 'Unknown error') }}</div>
        </div>
        {% elif result.matches %}
        <div class="matches">
            {% for match in result.matches %}
            <div class="match">
                <strong>Found "{{ match.keyword }}":</strong><br>
                {{ match.context }}
            </div>
            {% endfor %}
        </div>
        {% else %}
        <div class="matches">
            <div class="meta">No keyword matches found</div>
        </div>
        {% endif %}
    </div>
    {% endfor %}
    
    <div class="meta">
        Generated on {{ generation_time.strftime('%Y-%m-%d %H:%M:%S') }}
    </div>
</body>
</html>
    """
    
    def generate_report(self, results: List[Dict], keywords: List[str], days: int = None, cache_stats: Dict = None) -> str:
        """Generate HTML report from search results."""
        # Performance safeguard - limit results for browser rendering
        max_results_for_html = 100
        results_limited = len(results) > max_results_for_html
        if results_limited:
            print(f"WARNING: Limiting HTML report to first {max_results_for_html} results for performance")
            print(f"         Use --max-urls to reduce the number of URLs processed")
            results = results[:max_results_for_html]
        
        template = Template(self.HTML_TEMPLATE)
        
        successful_matches = sum(1 for r in results if r.get('matches'))
        failed_scrapes = sum(1 for r in results if r['status'] == 'error')
        from_cache_count = sum(1 for r in results if r.get('from_cache', False))
        fresh_scrape_count = len(results) - from_cache_count
        
        return template.render(
            results=results,
            keywords=keywords,
            days=days,
            total_urls=len(results),
            successful_matches=successful_matches,
            failed_scrapes=failed_scrapes,
            from_cache_count=from_cache_count,
            fresh_scrape_count=fresh_scrape_count,
            cache_stats=cache_stats or {},
            generation_time=datetime.now(),
            results_limited=results_limited
        )


async def scrape_and_cache_urls(urls: List[Dict], cache: ContentCache, 
                              force_refresh: bool = False, max_cache_age_hours: int = 24) -> List[Dict]:
    """Scrape URLs using cache when possible."""
    if not urls:
        return []
        
    scraper = AsyncWebScraper(cache)
    results = []
    
    cached_count = 0
    error_count = 0
    success_count = 0
    
    # Create all scraping tasks at once for parallel execution
    async def scrape_with_data(url_data):
        """Helper to scrape URL and return both result and original data."""
        url = url_data['url']
        use_cache = not force_refresh
        scraped = await scraper.scrape_url(url, use_cache, max_cache_age_hours)
        return url_data, scraped
    
    # Create tasks for all URLs
    tasks = [asyncio.create_task(scrape_with_data(url_data)) for url_data in urls]
    
    # Process completed tasks as they finish (up to max_concurrent at a time)
    with tqdm(total=len(urls), desc="Scraping URLs", unit="url") as pbar:
        for task in asyncio.as_completed(tasks):
            url_data, scraped = await task
            
            if scraped.get('from_cache'):
                cached_count += 1
                success_count += 1
                status = "cached"
            elif scraped['status'] == 'error':
                error_count += 1
                status = "error"
            else:
                success_count += 1
                status = "fresh"
            
            # Update progress bar with current stats
            pbar.set_postfix({
                'cached': cached_count,
                'fresh': success_count - cached_count, 
                'errors': error_count
            })
            pbar.update(1)
            
            # Add results
            results.append({
                **url_data,
                **scraped
            })
    
    await scraper.close()
    
    print(f"Scraping summary: {success_count}/{len(urls)} successful, {cached_count} from cache, {error_count} failed")
    return results


async def main():
    """Main application logic."""
    parser = argparse.ArgumentParser(description='Firefox history scraper with caching')
    
    # Common arguments
    parser.add_argument('keywords', nargs='*', help='Keywords to search for (required for search operations)')
    parser.add_argument('--days', type=int, default=7, help='Number of days to look back')
    parser.add_argument('--exclude', action='append', default=[], help='URL patterns to exclude (in addition to defaults)')
    parser.add_argument('--include', action='append', default=[], help='URL patterns to include (override exclusions)')
    parser.add_argument('--no-default-exclusions', action='store_true', help='Disable default exclusion patterns')
    parser.add_argument('--show-exclusions', action='store_true', help='Show active exclusion patterns and exit')
    parser.add_argument('--output', default='search_results.html', help='Output HTML file')
    parser.add_argument('--max-urls', type=int, default=1000, help='Maximum fresh URLs to scrape (cached URLs do not count, 0 for unlimited)')
    parser.add_argument('--cache-dir', type=Path, help='Cache directory path')
    parser.add_argument('--max-cache-age', type=int, default=24, help='Max cache age in hours')
    parser.add_argument('--profile', help='Firefox profile pattern (supports glob: "*", "default*", "work*")')
    
    # Operation modes (mutually exclusive)
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--list-profiles', action='store_true', 
                           help='List available Firefox profiles with pattern examples')
    mode_group.add_argument('--cache-stats', action='store_true', 
                           help='Show cache statistics')
    mode_group.add_argument('--clean-cache', type=int, metavar='DAYS', 
                           help='Remove cache entries older than DAYS')
    mode_group.add_argument('--search-cache', action='store_true', 
                           help='Search only cached content')
    mode_group.add_argument('--refresh-cache', action='store_true', 
                           help='Force refresh of all URLs and search')
    
    args = parser.parse_args()
    
    # Handle show exclusions (no other setup needed)
    if args.show_exclusions:
        print("Active Filtering Patterns:")
        
        if args.include:
            print("\nInclude Patterns (override exclusions):")
            for pattern in args.include:
                print(f"  + {pattern}")
        
        if not args.no_default_exclusions:
            print("\nDefault Domain Exclusions:")
            for pattern in DEFAULT_DOMAIN_EXCLUSIONS:
                print(f"  - {pattern}")
            print("\nDefault File Extension Exclusions:")
            for pattern in DEFAULT_FILE_EXTENSION_EXCLUSIONS:
                print(f"  - {pattern}")
        
        if args.exclude:
            print("\nCustom Exclusions:")
            for pattern in args.exclude:
                print(f"  - {pattern}")
        
        if args.no_default_exclusions and not args.exclude and not args.include:
            print("  - No filtering patterns active")
        
        return
    
    # Handle profile listing (no cache or keywords needed)
    if args.list_profiles:
        extractor = FirefoxHistoryExtractor()
        try:
            profiles = extractor.list_profiles()
            print("Available Firefox Profiles:")
            for profile in profiles:
                default_indicator = " (default)" if profile['is_default'] else ""
                print(f"  - {profile['name']}{default_indicator}")
                print(f"    Directory: {profile['directory']}")
            
            print("\nProfile Pattern Examples:")
            print("  --profile '*'         # Use any available profile (default)")
            print("  --profile 'default*'  # Use any profile starting with 'default'")
            if any('work' in p['name'].lower() for p in profiles):
                print("  --profile 'work*'     # Use any profile starting with 'work'")
            print("  --profile 'exact-name' # Use profile with exact directory name")
        except FileNotFoundError as e:
            print(f"Error: {e}")
        return
    
    # Initialize cache for all other operations
    cache = ContentCache(args.cache_dir)
    
    # Handle cache management commands
    if args.cache_stats:
        stats = cache.get_cache_stats()
        print("Cache Statistics:")
        print(f"  - Cache location: {cache.cache_dir}")
        print(f"  - Total entries: {stats['total_entries']}")
        print(f"  - Successful entries: {stats['successful_entries']}")
        print(f"  - Cache size: {stats['cache_size_mb']:.1f} MB")
        if stats['oldest_entry']:
            print(f"  - Date range: {stats['oldest_entry'][:10]} to {stats['newest_entry'][:10]}")
        return
    
    if args.clean_cache:
        deleted = cache.clean_old_entries(args.clean_cache)
        print(f"Cleaned {deleted} old cache entries from {cache.cache_dir}")
        return
    
    # For all search operations, keywords are required
    if not args.keywords:
        print("Error: Keywords required for search operations")
        print("Available operations: --list-profiles, --cache-stats, --clean-cache")
        return
    
    # Initialize extractor for search operations  
    extractor = FirefoxHistoryExtractor()
    
    # Prepare exclusion and inclusion patterns
    use_defaults = not args.no_default_exclusions
    patterns = get_effective_patterns(args.exclude, args.include, use_defaults)
    
    # Check for potentially problematic include patterns and warn
    check_include_warnings(args.include, patterns['exclusions'])
    
    # Show pattern info
    exclusions = patterns['exclusions']
    inclusions = patterns['inclusions']
    total_exclusions = len(exclusions['domains']) + len(exclusions['extensions']) + len(exclusions['general'])
    total_inclusions = len(inclusions['domains']) + len(inclusions['extensions']) + len(inclusions['general'])
    
    if total_exclusions > 0 or total_inclusions > 0:
        parts = []
        if total_exclusions > 0:
            parts.append(f"{total_exclusions} exclusions ({len(exclusions['domains'])} domains, {len(exclusions['extensions'])} extensions, {len(exclusions['general'])} custom)")
        if total_inclusions > 0:
            parts.append(f"{total_inclusions} inclusions")
        print(f"Using {', '.join(parts)}")
    
    # Show profile being used (resolve pattern first)
    try:
        if args.profile:
            # Resolve the pattern to get actual profile path
            resolved_path = extractor.get_profile_path(args.profile)
            profile_dir = resolved_path.parent.name
            print(f"Using Firefox profile: {profile_dir}")
        else:
            profiles = extractor.list_profiles()
            default_profile = next((p for p in profiles if p['is_default']), profiles[0])
            print(f"Using Firefox profile: {default_profile['name']}")
    except Exception as e:
        # If we can't resolve now, we'll get a better error later during extraction
        print(f"Using Firefox profile: {args.profile or 'default'}")
    
    # Handle search operations
    if args.search_cache:
        print(f"Searching cached content for: {', '.join(args.keywords)}")
        results = cache.search_cached_content(args.keywords, patterns)
        print(f"Found {len(results)} results in cache")
        
    else:
        # Regular operation (default) or refresh cache mode
        force_refresh = args.refresh_cache
        operation_type = "refresh and search" if force_refresh else "search"
        print(f"Extracting URLs from Firefox history for {operation_type} (last {args.days} days)...")
        
        try:
            urls = extractor.extract_urls(args.days, patterns, args.profile)
            print(f"Found {len(urls)} URLs")
            
            if args.max_urls > 0 and len(urls) > args.max_urls:
                urls = urls[:args.max_urls]
                print(f"Limited to {args.max_urls} most recent URLs")
            
        except FileNotFoundError as e:
            print(f"Error: {e}")
            print("Try --list-profiles to see available profiles")
            return
        except ValueError as e:
            print(f"Error: {e}")
            print("Use --list-profiles to see available profiles")
            return
        
        # Scrape URLs
        cache_mode = "Force refreshing" if force_refresh else "Processing"
        print(f"\n{cache_mode} {len(urls)} URLs...")
        
        scraped_results = await scrape_and_cache_urls(
            urls, cache, force_refresh, args.max_cache_age
        )
        
        # Search for keywords in scraped content
        print(f"\nSearching for keywords: {', '.join(args.keywords)}")
        keyword_searcher = KeywordSearcher()
        
        results = []
        successful_content = [r for r in scraped_results if r['status'] == 'success' and r['content']]
        
        with tqdm(successful_content, desc="Searching content", unit="page") as pbar:
            matches_found = 0
            for result in pbar:
                matches = keyword_searcher.search_content(result['content'], args.keywords)
                if matches:  # Only include results with matches
                    result['matches'] = matches
                    results.append(result)
                    matches_found += 1
                
                pbar.set_postfix({'matches': matches_found})
    
    # Generate HTML report for search operations
    print(f"\nGenerating HTML report...")
    
    generator = HTMLReportGenerator()
    cache_stats = cache.get_cache_stats()
    html_content = generator.generate_report(results, args.keywords, args.days, cache_stats)
    
    # Write report
    async with aiofiles.open(args.output, 'w', encoding='utf-8') as f:
        await f.write(html_content)
    
    # Summary
    matches_found = sum(len(r['matches']) for r in results)
    
    print(f"\nComplete!")
    print(f"Summary:")
    print(f"  - {matches_found} keyword matches found")
    print(f"  - {len(results)} URLs contained matches")
    print(f"  - Report saved to: {args.output}")


if __name__ == "__main__":
    asyncio.run(main())
